{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54e0c6dc80e94c76bffe10bde3ba7a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eea61abe5ecb40f6ad0eabef3bd2237c",
              "IPY_MODEL_542a669a6bc04d57afe73c204e394bab",
              "IPY_MODEL_2f7c090900c54784b9ee5b65b6d5d13f"
            ],
            "layout": "IPY_MODEL_231492b344f54f518343f3a2e9aa9f9b"
          }
        },
        "eea61abe5ecb40f6ad0eabef3bd2237c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b256be1a68438ba8e58fa87e88a577",
            "placeholder": "​",
            "style": "IPY_MODEL_2b4b6080b17b44bd903f40a8f6879e96",
            "value": ""
          }
        },
        "542a669a6bc04d57afe73c204e394bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12e3adb395d42d0a01b121aa41c90e4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dedef94d2ab342deae21ac3e724eb38e",
            "value": 0
          }
        },
        "2f7c090900c54784b9ee5b65b6d5d13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3225d10b6944101acb5f11645f5f81b",
            "placeholder": "​",
            "style": "IPY_MODEL_c5a7f0019933448cb5afa0ccaa3e74f6",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "231492b344f54f518343f3a2e9aa9f9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b256be1a68438ba8e58fa87e88a577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b4b6080b17b44bd903f40a8f6879e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c12e3adb395d42d0a01b121aa41c90e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dedef94d2ab342deae21ac3e724eb38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3225d10b6944101acb5f11645f5f81b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a7f0019933448cb5afa0ccaa3e74f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs and imports"
      ],
      "metadata": {
        "id": "a6cPGzEKVVkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpYqVJfk1Icl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7b7401-b6f6-4998-82d8-bfe453f47dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/484.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install bitsandbytes accelerate datasets --quiet\n",
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install vllm"
      ],
      "metadata": {
        "id": "EWAwPM7t6haJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sgl-kernel --force-reinstall --no-deps\n",
        "!pip install \"sglang[all]>=0.4.2.post2\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer/"
      ],
      "metadata": {
        "id": "WQZTHYGMQIHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjFFsn9QXlAx",
        "outputId": "0e964b1e-4fa3-48bb-c429-2063dfe9d304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from random import randint\n",
        "import pickle, json, time, torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CKkkfT8sl5AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the DATA"
      ],
      "metadata": {
        "id": "ryGMH2LaVdns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1209 #97"
      ],
      "metadata": {
        "id": "n39x2qDVa754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = load_dataset(\"cais/mmlu\", \"all\").shuffle(seed=seed)\n",
        "bn = load_dataset(\"openai/MMMLU\", \"BN_BD\").shuffle(seed=seed)\n",
        "sw = load_dataset(\"openai/MMMLU\", \"SW_KE\").shuffle(seed=seed)\n",
        "ja = load_dataset(\"openai/MMMLU\", \"JA_JP\").shuffle(seed=seed)\n",
        "yo = load_dataset(\"openai/MMMLU\", \"YO_NG\").shuffle(seed=seed)\n",
        "id = load_dataset(\"openai/MMMLU\", \"ID_ID\").shuffle(seed=seed)"
      ],
      "metadata": {
        "id": "IIYEp-jWYD5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 1000#200\n",
        "\n",
        "en_data = en[\"test\"][\"question\"][:n_samples], en[\"test\"][\"choices\"][:n_samples], en[\"test\"][\"answer\"][:n_samples]\n",
        "bn_data = bn[\"test\"][:n_samples][\"Question\"], list(zip(bn[\"test\"][:n_samples][\"A\"], bn[\"test\"][:n_samples][\"B\"], bn[\"test\"][:n_samples][\"C\"], bn[\"test\"][:n_samples][\"D\"])), bn[\"test\"][:n_samples][\"Answer\"]\n",
        "yo_data = yo[\"test\"][:n_samples][\"Question\"], list(zip(yo[\"test\"][:n_samples][\"A\"], yo[\"test\"][:n_samples][\"B\"], yo[\"test\"][:n_samples][\"C\"], yo[\"test\"][:n_samples][\"D\"])), yo[\"test\"][:n_samples][\"Answer\"]\n",
        "sw_data = sw[\"test\"][:n_samples][\"Question\"], list(zip(sw[\"test\"][:n_samples][\"A\"], sw[\"test\"][:n_samples][\"B\"], sw[\"test\"][:n_samples][\"C\"], sw[\"test\"][:n_samples][\"D\"])), sw[\"test\"][:n_samples][\"Answer\"]\n",
        "jp_data = ja[\"test\"][:n_samples][\"Question\"], list(zip(ja[\"test\"][:n_samples][\"A\"], ja[\"test\"][:n_samples][\"B\"], ja[\"test\"][:n_samples][\"C\"], ja[\"test\"][:n_samples][\"D\"])), ja[\"test\"][:n_samples][\"Answer\"]\n",
        "id_data = id[\"test\"][:n_samples][\"Question\"], list(zip(id[\"test\"][:n_samples][\"A\"], id[\"test\"][:n_samples][\"B\"], id[\"test\"][:n_samples][\"C\"], id[\"test\"][:n_samples][\"D\"])), id[\"test\"][:n_samples][\"Answer\"]\n",
        "\n",
        "aux_langs_dict = {\n",
        "    \"high_res\": ['eng_Latn', 'deu_Latn', 'fra_Latn', 'spa_Latn', 'zho_Hans', 'por_Latn'],\n",
        "    \"mid_res\": ['ell_Grek', 'heb_Hebr', 'hin_Deva', 'ind_Latn', 'ukr_Cyrl', 'vie_Latn'],\n",
        "    \"low_res\": ['tel_Telu', 'npi_Deva', 'mai_Deva', 'bho_Deva', 'yor_Latn', 'zul_Latn']\n",
        "}\n",
        "\n",
        "tgt_lang_data = {\n",
        "    \"ben_Beng\": (\"Bengali\", bn_data),\n",
        "    \"eng_Latn\": (\"English\", en_data),\n",
        "    \"yor_Latn\": (\"Yoruba\", yo_data),\n",
        "    \"swh_Latn\": (\"Swahili\", sw_data),\n",
        "    \"jpn_Jpan\": (\"Japanese\", jp_data),\n",
        "    \"ind_Latn\": (\"Indonesian\", id_data)\n",
        "}"
      ],
      "metadata": {
        "id": "voomlLCjl0tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLOBALS"
      ],
      "metadata": {
        "id": "xr8DlXkIboMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Given below is a question and possible answers. Choose the correct answer.\\n\\n\"\n",
        "# instruction = \"Given below is a question, possible choices and the correct answer.\\n\\n\"\n",
        "\n",
        "prompt_in_tgt = False\n",
        "batch_size = 16\n",
        "max_new_tokens = 32\n",
        "\n",
        "# prompt_model = \"gemma-2-2b-it\"\n",
        "# prompt_model_name = prompt_model.split(\"/\")[1]\n",
        "\n",
        "# prompt_model = \"gemma-2-9b-it\""
      ],
      "metadata": {
        "id": "mtFojRFJboSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common HELPERS"
      ],
      "metadata": {
        "id": "P8avjEC3Vh-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_batched(\n",
        "  model,\n",
        "  tokenizer,\n",
        "  prompts,\n",
        "  batch_size = 8,\n",
        "  max_length = 768,\n",
        "  max_new_tokens = 64,\n",
        "  num_return_sequences = 1,\n",
        "  temperature = 0.7,\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "  all_generated_texts = []\n",
        "\n",
        "  for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "    batch_prompts = prompts[i:i + batch_size]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "      batch_prompts,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=max_new_tokens,\n",
        "          temperature=temperature,\n",
        "          # top_p=top_p,\n",
        "          num_return_sequences=num_return_sequences,\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          eos_token_id=tokenizer.eos_token_id,\n",
        "          do_sample=True\n",
        "      )\n",
        "\n",
        "      if num_return_sequences > 1:\n",
        "          outputs = outputs.view(len(batch_prompts), num_return_sequences, -1)\n",
        "          batch_texts = [\n",
        "              [tokenizer.decode(output, skip_special_tokens=True)\n",
        "                for output in prompt_outputs]\n",
        "              for prompt_outputs in outputs\n",
        "          ]\n",
        "      else:\n",
        "          batch_texts = [\n",
        "              [tokenizer.decode(output, skip_special_tokens=True)]\n",
        "              for output in outputs\n",
        "          ]\n",
        "\n",
        "      all_generated_texts.extend(batch_texts)\n",
        "\n",
        "  return all_generated_texts\n",
        "\n",
        "\n",
        "def translate_to_tgt_batched(\n",
        "  texts,\n",
        "  src_lang,\n",
        "  tgt_lang,\n",
        "  batch_size = 32,\n",
        "  max_length = 384,\n",
        "  num_beams = 5,\n",
        "  temperature = 0.7,\n",
        "  device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "  all_translated_texts = []\n",
        "  nllb_tokenizer.src_lang = src_lang\n",
        "  forced_bos_token_id = nllb_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "\n",
        "  for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "    inputs = nllb_tokenizer(\n",
        "      batch_texts,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "    ).to(device)\n",
        "\n",
        "    inputs[\"forced_bos_token_id\"] = forced_bos_token_id\n",
        "\n",
        "    with torch.no_grad():\n",
        "      translated = nllb.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature\n",
        "      )\n",
        "\n",
        "      batch_translated_texts = nllb_tokenizer.batch_decode(\n",
        "        translated,\n",
        "        skip_special_tokens=True\n",
        "      )\n",
        "\n",
        "      all_translated_texts.extend(batch_translated_texts)\n",
        "\n",
        "  return all_translated_texts\n",
        "\n",
        "\n",
        "def get_answers(model, tokenizer, prompts, add_instruction=False):\n",
        "  EOS_TOKEN = tokenizer.eos_token\n",
        "  for i in range(len(prompts)):\n",
        "      for j in range(len(prompts[i])):\n",
        "          prompts[i][j] += EOS_TOKEN\n",
        "\n",
        "  instr = instruction if add_instruction else \"\"\n",
        "\n",
        "  prompts_list = [list(item) for item in zip(*prompts)]\n",
        "  prompts_flatten = [instr + item for sublist in prompts_list for item in sublist]\n",
        "  # print(prompts_flatten[:5])\n",
        "\n",
        "  answers = generate_text_batched(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts_flatten,\n",
        "    batch_size=batch_size,\n",
        "    max_new_tokens=32\n",
        "  )\n",
        "\n",
        "  return answers, prompts_list\n",
        "\n",
        "\n",
        "def prompt_model_sg(prompts, chunks, add_instruction=False):\n",
        "  instr = instruction if add_instruction else \"\"\n",
        "\n",
        "  prompts_list = [list(item) for item in zip(*prompts)]\n",
        "  prompts_flatten = [instr + item for sublist in prompts_list for item in sublist]\n",
        "  # print(prompts_flatten[:5])\n",
        "\n",
        "  answers = sg_generate(prompts_flatten)\n",
        "\n",
        "  answers = [answer[\"text\"].strip() for answer in answers] # SG\n",
        "\n",
        "  # answers = [answer.outputs[0].text.strip() for answer in answers] #vLLM\n",
        "\n",
        "  answers_chunks = list(chunk(answers, chunks))\n",
        "  answers_only = []\n",
        "\n",
        "  no_answers = []\n",
        "  for answer_chunk in answers_chunks:\n",
        "    answers_per_lang = []\n",
        "    no_answers_curr_lang = 0\n",
        "    for answer in answer_chunk:\n",
        "      try:\n",
        "        A = answer.split(\"\\n\\n\")[0]\n",
        "        answers_per_lang.append(A)\n",
        "      except Exception as e:\n",
        "        answers_per_lang.append(answer)\n",
        "        no_answers_curr_lang += 1\n",
        "\n",
        "    no_answers.append(no_answers_curr_lang)\n",
        "    answers_only.append(answers_per_lang)\n",
        "\n",
        "  # print(\"No answers for: \", sum(no_answers))\n",
        "  return answers_only, prompts_list\n",
        "\n",
        "\n",
        "def chunk(lst, n):\n",
        "  for i in range(0, len(lst), n):\n",
        "    yield lst[i:i + n]\n",
        "\n",
        "\n",
        "def extract_answers(answers, chunks):\n",
        "  answers_chunks = list(chunk(answers, chunks))\n",
        "\n",
        "  answers_only = []\n",
        "\n",
        "  no_answers = []\n",
        "  for answer_chunk in answers_chunks:\n",
        "    answers_per_lang = []\n",
        "    no_answers_curr_lang = 0\n",
        "    for answer in answer_chunk:\n",
        "      try:\n",
        "        A = answer[0].split(\"Answer:\")[1]\n",
        "        try:\n",
        "          answers_per_lang.append(A.split(\"\\n\\n\")[0].strip())\n",
        "        except Exception as e:\n",
        "          answers_per_lang.append(A)\n",
        "      except Exception as e:\n",
        "        answers_per_lang.append(\"\")\n",
        "        no_answers_curr_lang += 1\n",
        "\n",
        "    no_answers.append(no_answers_curr_lang)\n",
        "    answers_only.append(answers_per_lang)\n",
        "\n",
        "  print(\"No answers for: \", sum(no_answers))\n",
        "  return answers_only\n",
        "\n",
        "\n",
        "def save_to_pickle(obj, file_name):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def target_to_auxiliary(prompts, options, answers, tgt_lang, aux_langs, max_length):\n",
        "  translated_prompts = []\n",
        "  translated_df = pd.DataFrame({'prompt': prompts, 'option': [str(opts) for opts in options], 'answer': answers})\n",
        "\n",
        "  for aux_lang in tqdm(aux_langs):\n",
        "    prompt_translations = translate_to_tgt_batched_ctranslate(\n",
        "      prompts, tgt_lang, aux_lang, batch_size=batch_size, max_length=max_length\n",
        "    )\n",
        "    options_translations = translate_to_tgt_batched_ctranslate(\n",
        "      [str(opts) for opts in options], tgt_lang, aux_lang, batch_size=batch_size, max_length=max_length\n",
        "    )\n",
        "\n",
        "    translated_prompts.append(\n",
        "      [f\"Question: {p[:512]}\\n\\nChoices: {o[:512]}\\n\\nCorrect Answer: \" for p, o in zip(prompt_translations, options_translations)]\n",
        "    )\n",
        "\n",
        "    translated_df[f\"prompt_{aux_lang}\"] = prompt_translations\n",
        "    translated_df[f\"options_{aux_lang}\"] = options_translations\n",
        "\n",
        "  if prompt_in_tgt:\n",
        "    translated_prompts.append([f\"Question: {p}\\n\\nChoices: {o}\\n\\nCorrect Answer: \" for p, o in zip(prompts, options)])\n",
        "\n",
        "  return translated_prompts, translated_df\n",
        "\n",
        "\n",
        "def auxiliary_to_target(extracted_answers, aux_langs, max_length):\n",
        "  answers_aux_lang = list(zip(*extracted_answers))\n",
        "  # tgt_lang_answers = answers_aux_lang.pop(-1) # Don't translate the answers in the tgt language\n",
        "\n",
        "  answers_aux_lang = zip(answers_aux_lang, aux_langs)\n",
        "  answer_translations = {}\n",
        "\n",
        "  for p, l in tqdm(answers_aux_lang):\n",
        "    answer_translations[l] = translate_to_tgt_batched_ctranslate(\n",
        "      p, l, tgt_lang, batch_size=batch_size, max_length=max_length\n",
        "    )\n",
        "\n",
        "  if prompt_in_tgt:\n",
        "    answer_translations.append(tgt_lang_answers)\n",
        "\n",
        "  return answer_translations\n",
        "\n",
        "\n",
        "def handle_null(obj):\n",
        "    if obj is None:\n",
        "        return ''\n",
        "    return obj\n",
        "\n",
        "\n",
        "def get_prompts_from_df(df):\n",
        "  # Only creating this function in case the prompt structure needs to be changed\n",
        "  # Make changes in the f-strings below\n",
        "  translated_prompts_ = []\n",
        "\n",
        "  EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "  for aux_lang in tqdm(aux_langs):\n",
        "    prompt_translations = df[f\"prompt_{aux_lang}\"]\n",
        "    options_translations = df[f\"options_{aux_lang}\"]\n",
        "    translated_prompts_.append([f\"Question: {p}\\n\\nChoices: {o}\\n\\nCorrect Answer: \" for p, o in zip(prompt_translations, options_translations)])\n",
        "\n",
        "  translated_prompts_.append([f\"Question: {p}\\n\\nChoices: {o}\\n\\nCorrect Answer: \" for p, o in zip(prompts, options)])\n",
        "\n",
        "  return translated_prompts_\n",
        "\n",
        "\n",
        "def load_pickle(file_path):\n",
        "  with open(file_path, 'rb') as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "\n",
        "def translate_to_tgt_batched_ctranslate(source, src_lang, tgt_lang, batch_size=32, max_length=256):\n",
        "  all_translated_texts = []\n",
        "  nllb_tokenizer.src_lang = src_lang\n",
        "  target_prefix = [tgt_lang]\n",
        "\n",
        "  texts = [nllb_tokenizer.convert_ids_to_tokens(nllb_tokenizer.encode(sent)) for sent in source]\n",
        "\n",
        "  forced_bos_token_id = nllb_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "\n",
        "  for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      results = nllb.translate_batch(batch_texts, target_prefix=[target_prefix]*len(batch_texts), max_decoding_length=max_length)\n",
        "\n",
        "      targets = [result.hypotheses[0][1:] for result in results]\n",
        "\n",
        "      outputs = [nllb_tokenizer.decode(nllb_tokenizer.convert_tokens_to_ids(target_)) for target_ in targets]\n",
        "\n",
        "      all_translated_texts.extend(outputs)\n",
        "\n",
        "  return all_translated_texts\n",
        "\n",
        "\n",
        "options_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "\n",
        "def process_answers(model_answers_translated, prompts, options, true_answers, prompts_list, model_responses, aux_langs):\n",
        "  model_answers_by_prompt = list(zip(*model_answers_translated))\n",
        "\n",
        "  confidences, ground_truths, samples = [], [], []\n",
        "\n",
        "  # Maybe try to vectorize this bit as well\n",
        "  for i, (answers, correct_answer) in tqdm(enumerate(zip(model_answers_by_prompt, true_answers))):\n",
        "    true_answer = options_map[correct_answer] if tgt_lang != \"eng_Latn\" else correct_answer\n",
        "    true_answer_string = options[i][true_answer]\n",
        "\n",
        "    final_answers_set = list(answers).copy()\n",
        "\n",
        "    try:\n",
        "      correct_idx_in_answers = find_most_probable_answer(answers)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(answers)\n",
        "\n",
        "    selected_model_answer = final_answers_set.pop(correct_idx_in_answers)\n",
        "\n",
        "    answer_similarity = get_similarity([selected_model_answer]*len(final_answers_set), final_answers_set)\n",
        "    adjusted_sims = np.where(answer_similarity > 0.8, answer_similarity * 1.5, answer_similarity)\n",
        "    confidence = adjusted_sims.mean()\n",
        "    confidence = 1 if confidence > 1 else confidence\n",
        "\n",
        "    confidences.append(confidence)\n",
        "\n",
        "    true_v_model = get_similarity([selected_model_answer], [true_answer_string]).item()\n",
        "    correct = true_v_model > 0.85\n",
        "    ground_truths.append(correct)\n",
        "\n",
        "    samples.append({\n",
        "      \"question_id\": i,\n",
        "      \"original_question\": prompts[i],\n",
        "      \"translated_prompts\": {lang: question for lang, question in zip(aux_langs, prompts_list[i])},\n",
        "      \"responses\": {lang: answer for lang, answer in zip(aux_langs, model_responses[i])},\n",
        "      \"translated_responses\": {lang: answer for lang, answer in zip(aux_langs, model_answers_by_prompt[i])},\n",
        "      \"translated_responses_with_scores\": list(zip(final_answers_set, answer_similarity)),\n",
        "      \"decision\": {\n",
        "          \"final_answer\": selected_model_answer,\n",
        "          \"true_answer\": true_answer_string,\n",
        "          \"confidence\": confidence,\n",
        "          \"similarity_with_truth\": true_v_model,\n",
        "          \"correct\": correct\n",
        "      }\n",
        "    })\n",
        "\n",
        "  return confidences, ground_truths, samples"
      ],
      "metadata": {
        "id": "ZQEtvBmmZOh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# sentence_transformer = SentenceTransformer('LaBSE')\n",
        "sentence_transformer = SentenceTransformer('all-mpnet-base-v2')\n",
        "# sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def get_similarity(batch1, batch2):\n",
        "  embedding1 = sentence_transformer.encode(batch1)\n",
        "  embedding2 = sentence_transformer.encode(batch2)\n",
        "\n",
        "  similarities = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "  if len(batch1) == len(batch2):\n",
        "      return np.diag(similarities)\n",
        "  else:\n",
        "    return similarities[0]"
      ],
      "metadata": {
        "id": "qGSVa83Ulff5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MODELS"
      ],
      "metadata": {
        "id": "zJPPH4hEVnAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSrzKGUDDu8K",
        "outputId": "be7e21f3-5238-4b6c-ccdb-c235e6fc5e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/My Drive/Hallucination\\ AS"
      ],
      "metadata": {
        "id": "Dd69qmn0mHkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e4f68d-8b81-4bff-9fd1-dcc7df45d3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/14oaBn3SD6kuOrY0Do9QBRRm04hjUZ0wz/Hallucination AS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "WOyS-f8pmRzj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "54e0c6dc80e94c76bffe10bde3ba7a61",
            "eea61abe5ecb40f6ad0eabef3bd2237c",
            "542a669a6bc04d57afe73c204e394bab",
            "2f7c090900c54784b9ee5b65b6d5d13f",
            "231492b344f54f518343f3a2e9aa9f9b",
            "f7b256be1a68438ba8e58fa87e88a577",
            "2b4b6080b17b44bd903f40a8f6879e96",
            "c12e3adb395d42d0a01b121aa41c90e4",
            "dedef94d2ab342deae21ac3e724eb38e",
            "c3225d10b6944101acb5f11645f5f81b",
            "c5a7f0019933448cb5afa0ccaa3e74f6"
          ]
        },
        "outputId": "1700fab8-c616-44f0-dbd5-245fd7e1a61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54e0c6dc80e94c76bffe10bde3ba7a61"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If need to log in to access gated a model\n",
        "!huggingface-cli login --token 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\n",
        "# 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"
      ],
      "metadata": {
        "id": "2aZFiVKRnNwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255c05f8-bbb1-444f-e1be-e604df0d228d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `write-access` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `write-access`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_model = \"ssg97/gemma-2-27b-it-gptq-int4\"#\"Qwen/Qwen2.5-7B-Instruct\"#\"google/gemma-2-9b-it\"#\"Qwen/Qwen2.5-7B-Instruct\"#\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"#\"google/gemma-2-2b-it\"#\"google/gemma-2-9b-it\"#\"google/gemma-2-2b-it\"#\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\"ssg97/gemma-2-27b-it-gptq-int4\"#\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"#\"CohereForAI/aya-expanse-8b\"\n",
        "prompt_model_name = prompt_model.split(\"/\")[1]"
      ],
      "metadata": {
        "id": "L0JmTe7kKWBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGLang\n",
        "\n",
        "# Failed: 0.4, 0.6, 0.7, 0.75 Working (L4): 0.9 >> 0.8 (18.8/22.5 for QWEN 7B at .8)\n",
        "\n",
        "from sglang import Engine\n",
        "import torch\n",
        "\n",
        "llm = Engine(model_path=prompt_model, mem_fraction_static=0.8, dtype=torch.bfloat16)#mem_fraction_static=.8, quantization=\"gptq_marlin\")#, dtype=torch.bfloat16)\n",
        "sglang_params = {\"temperature\": 0.7, \"top_p\": 0.95, \"max_new_tokens\": 32}\n",
        "\n",
        "sg_generate = lambda prompts: llm.generate(prompts, sglang_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UqCVui8he3A",
        "outputId": "7ff7621f-d905-4c2a-ddfe-4b6f18790a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-14 10:09:08 __init__.py:190] Automatically detected platform cuda.\n",
            "INFO 02-14 10:09:12 gptq_marlin.py:111] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "sglang_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=32)\n",
        "llm = LLM(model=prompt_model, max_num_seqs=20, dtype=torch.bfloat16)\n",
        "\n",
        "sg_generate = lambda prompts: llm.generate(prompts, sglang_params)\n",
        "          # trust_remote_code=True, \\\n",
        "# quantization=\"bitsandbytes\", load_format=\"bitsandbytes\")"
      ],
      "metadata": {
        "id": "n0EMsiKmSnG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.shutdown()"
      ],
      "metadata": {
        "id": "lDOydFLm-ot_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_model = \"aya-expanse-4-bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(prompt_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(prompt_model, device_map=\"auto\")\n",
        "\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "TBsJah0zmFpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctranslate2"
      ],
      "metadata": {
        "id": "IqqHSN9PmQCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHLbZp2hqLzZ",
        "outputId": "84c065d1-f223-4ef5-f6e2-0a35af34c776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HAS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ctranslate2\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = \"cuda\"\n",
        "translation_model = \"nllb-200-distilled-1.3B-int8\"\n",
        "\n",
        "nllb = ctranslate2.Translator(translation_model, device)\n",
        "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")"
      ],
      "metadata": {
        "id": "Ncm0AXwqmI3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "similarity_model = 'paraphrase-multilingual-mpnet-base-v2' #'all-mpnet-base-v2'\n",
        "sentence_transformer = SentenceTransformer(similarity_model)\n",
        "\n",
        "def get_similarity(batch1, batch2):\n",
        "    embedding1 = sentence_transformer.encode(batch1)\n",
        "    embedding2 = sentence_transformer.encode(batch2)\n",
        "\n",
        "    similarities = cosine_similarity(embedding1, embedding2)\n",
        "\n",
        "    if len(batch1) == len(batch2):\n",
        "        return np.diag(similarities)\n",
        "    else:\n",
        "      return similarities[0]"
      ],
      "metadata": {
        "id": "i3N10rVFo9L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE Results: without the MKA"
      ],
      "metadata": {
        "id": "xa-eX3guVrrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/My Drive/HAS/"
      ],
      "metadata": {
        "id": "esrUrwKamjaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a071a397-c21b-4187-98b7-73d539405f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HAS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktjdE0ATmccu",
        "outputId": "96155fbd-a137-43b4-d04d-22f7c483d514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HAS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this function to use a different correct answer calibration\n",
        "def get_model_correctness(model_answers, true_answers):\n",
        "  return get_similarity(model_answers, true_answers) > 0.85"
      ],
      "metadata": {
        "id": "J_KeQLBQsH-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt model and get answers\n",
        "\n",
        "prompts = []\n",
        "add_instruction = True\n",
        "batch_size = 32\n",
        "\n",
        "for tgt_lang, (lang_folder, (questions, options, answers)) in tgt_lang_data.items():\n",
        "  prompts.append([f\"Question: {p[:512]}\\n\\nChoices: {o[:512]}\\n\\nCorrect Answer: \" for p, o in zip(questions, options)])\n",
        "\n",
        "# answers, prompts_list = get_answers(model, tokenizer, prompts, add_instruction=add_instruction)\n",
        "# print(len(prompts))\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "answers, prompts_list = prompt_model_sg(prompts, 6, add_instruction=add_instruction)\n",
        "print(f\"{time.time() - start} seconds\")\n",
        "\n",
        "save_to_pickle(answers, f\"Baseline/{prompt_model_name}_responses_{n_samples}.pkl\")"
      ],
      "metadata": {
        "id": "ShBcS1Mw1MzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892de2a6-fb1a-40d1-88fe-c383ab789418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "315.1069858074188 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "metadata": {
        "id": "EgyNsoGm5QIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prompts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUcOdpu0R-Vc",
        "outputId": "0a995bbc-7cae-4a73-d64f-73bedea7a2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompting the model...\n",
        "# 100%|██████████| 75/75 [17:30<00:00, 14.01s/it]\n",
        "# No answers for:  34\n",
        "\n",
        "# Processed prompts: 100%|██████████| 1200/1200 [01:24<00:00, 14.16it/s, est. speed input: 2337.88 toks/s, output: 453.03 toks/s]\n",
        "\n",
        "# 26.180737495422363 seconds"
      ],
      "metadata": {
        "id": "GQJ64GyDT29z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answers = load_pickle(f\"Baseline/{prompt_model_name}_responses_{n_samples}.pkl\")\n",
        "\n",
        "# answers_only = extract_answers(answers, 6)\n",
        "model_answers_language_wise = list(zip(*answers))"
      ],
      "metadata": {
        "id": "TWGOCM4_mbdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get confidences and accuracy\n",
        "\n",
        "options_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "\n",
        "for i, (tgt_lang, (lang_folder, (prompts, options, true_answers))) in enumerate(tgt_lang_data.items()):\n",
        "  model_answers = list(model_answers_language_wise[i])\n",
        "\n",
        "  true_answers_idx = []\n",
        "  for correct in true_answers:\n",
        "    if tgt_lang != \"eng_Latn\":\n",
        "      true_answers_idx.append(options_map[correct])\n",
        "    else:\n",
        "      true_answers_idx.append(correct)\n",
        "\n",
        "  true_answers_strings = [options[i][idx] for i, idx in enumerate(true_answers_idx)]\n",
        "\n",
        "  print(model_answers[1], true_answers_strings[1])\n",
        "  model_correctness = get_model_correctness(model_answers, true_answers_strings)\n",
        "  accuracy = np.mean(model_correctness)\n",
        "\n",
        "  print(f\"{tgt_lang}: {accuracy}\")"
      ],
      "metadata": {
        "id": "KzL9-klWhabT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 1000\n",
        "# Gemma2 9B:\n"
      ],
      "metadata": {
        "id": "Z5jScyXIuilf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 200\n",
        "# Gemma2 9B\n",
        "[0.225, 0.395, 0.065, 0.12, 0.265, 0.28]\n",
        "# Gemma2 9B with paraphrase-multilingual judge\n",
        "[0.105, 0.51, 0.12, 0.14, 0.285, 0.345]\n",
        "\n",
        "# Gemma2 2B\n",
        "[0.13, 0.205, 0.035, 0.09, 0.185, 0.155]\n",
        "# Gemma2 2B paraphrase\n",
        "[0.055, 0.405, 0.08, 0.11, 0.245, 0.215]\n",
        "\n",
        "# Qwen\n",
        "[0.5, 0.345, 0.255, 0.25, 0.39, 0.375]\n",
        "# Qwen para\n",
        "[0.245, 0.435, 0.325, 0.205, 0.41, 0.365]\n",
        "\n",
        "# Aya\n",
        "[0.415, 0.5, 0.265, 0.195, 0.515, 0.38]\n",
        "# Aya para\n",
        "[0.16, 0.51, 0.39, 0.285, 0.515, 0.435]\n",
        "\n",
        "# Gemma 27B\n",
        "[0.155, 0.62, 0.335, 0.245, 0.45, 0.435]"
      ],
      "metadata": {
        "id": "E8HuClLXFX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aya Expanse 4 bit\n",
        "baseline_accuracy_50_samples = [.46, .58, .36, .36, .66, .58]\n",
        "baseline_accuracy_200_samples = [0.475, 0.58, 0.295, 0.275, 0.51, 0.45]\n",
        "# Aya Expanse 8 FULL SG LANG: [0.415, 0.5, 0.265, 0.195, 0.515, 0.38]"
      ],
      "metadata": {
        "id": "GlZNYqx7lgCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The MKA Pipeline"
      ],
      "metadata": {
        "id": "cn2mz3HoVwAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/My Drive/HAS/MKA-SG-1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hig1e23L32QW",
        "outputId": "18be5178-b98d-4088-e05b-761326ad234e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HAS/MKA-SG-1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create directories for the six target languages, with each folder also containing 'intermediate_files' and 'results' folders\n",
        "\n",
        "import os\n",
        "\n",
        "# target_languages, _ = tgt_lang_data.items()\n",
        "\n",
        "for l, (lang, _) in tgt_lang_data.items():\n",
        "  os.makedirs(lang, exist_ok=True)\n",
        "  os.makedirs(os.path.join(lang, \"intermediate_files\"), exist_ok=True)\n",
        "  os.makedirs(os.path.join(lang, \"results\"), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "qDOgso3VrRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing prompts in auxiliary languages...\n",
        "# Batch\n",
        "# ben_Beng  ==>  ['deu_Latn', 'fra_Latn', 'spa_Latn', 'zho_Hans', 'rus_Cyrl', 'por_Latn']\n",
        "# 100%|██████████| 6/6 [04:27<00:00, 44.66s/it]\n",
        "# Batch 64\n",
        "# 17%|█▋        | 1/6 [00:36<03:03, 36.77s/it]\n",
        "\n",
        "print(\"Preparing prompts in auxiliary languages...\")\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "for tgt_lang, (lang, (prompts, options, answers)) in tgt_lang_data.items():\n",
        "  if tgt_lang in [\"ben_Beng\", \"eng_Latn\", \"yor_Latn\", \"swh_Latn\"]:\n",
        "    continue\n",
        "  for task, aux_langs in aux_langs_dict.items():\n",
        "    # if task in [\"high_res\", \"mid_res\"]:\n",
        "    #   continue\n",
        "    print(tgt_lang, \" ==> \", aux_langs)\n",
        "\n",
        "    max_length = 384 if task == \"low_res\" else 256\n",
        "\n",
        "    translated_prompts, translated_df = target_to_auxiliary(\n",
        "      prompts,\n",
        "      options,\n",
        "      answers,\n",
        "      tgt_lang,\n",
        "      aux_langs,\n",
        "      max_length=max_length\n",
        "    )\n",
        "\n",
        "    translated_df.to_json(\n",
        "      f'{lang}/intermediate_files/{task}_{n_samples}_translated_prompts.jsonl',\n",
        "      orient='records',\n",
        "      lines=True\n",
        "    )\n",
        "\n",
        "    save_to_pickle(\n",
        "      translated_prompts,\n",
        "      f'{lang}/intermediate_files/{task}_{n_samples}_translated_prompts.pkl'\n",
        "    )\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "F5RkFhal1M7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4h 32m 26s for 200 samples (6 tgt languages, 6 aux languages = 200*6*6*3 = 21600 inferences) - need to make it quicker?\n",
        "# 1.7 inferences/s -- looks slow!\n",
        "add_instruction = True\n",
        "print(prompt_model_name)\n",
        "\n",
        "# batch_size = 8\n",
        "\n",
        "print(\"Prompting the model...\")\n",
        "\n",
        "for tgt_lang, (lang, (prompts, options, answers)) in tgt_lang_data.items():\n",
        "  # if tgt_lang in [\"ben_Beng\"]:\n",
        "  #   continue\n",
        "  # else:\n",
        "  print(tgt_lang)\n",
        "  for task, aux_langs in tqdm(aux_langs_dict.items()): # tqdm for SGLang\n",
        "    translated_prompts = load_pickle(f'{lang}/intermediate_files/{task}_{n_samples}_translated_prompts.pkl')\n",
        "\n",
        "    answers, prompts_list = prompt_model_sg(translated_prompts, len(aux_langs), add_instruction=add_instruction)\n",
        "\n",
        "    save_to_pickle(answers, f'{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_prompt_answers.pkl')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "  # break"
      ],
      "metadata": {
        "id": "nL-C_afzBSds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# also use gemma-2-27b-it fp8 to compare 2b, 9b and 27b fp8"
      ],
      "metadata": {
        "id": "ZvI-exxYrwFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "from vllm.distributed.parallel_state import (\n",
        "    destroy_model_parallel,\n",
        "    destroy_distributed_environment,\n",
        ")\n",
        "\n",
        "# destroy_model_parallel()\n",
        "# destroy_distributed_environment()\n",
        "# del llm.llm_engine.model_executor\n",
        "# del llm\n",
        "# with contextlib.suppress(AssertionError):\n",
        "  # torch.distributed.destroy_process_group()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# ray.shutdown()"
      ],
      "metadata": {
        "id": "ckpigmFhQhV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: copy files from ../MKA-200/English/intermediate_files/ to English/intermediate_files, make folders if they don't exist\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source and destination directories\n",
        "languages = [\"Bengali\", \"English\", \"Japanese\", \"Yoruba\", \"Swahili\", \"Indonesian\"]\n",
        "\n",
        "for language in languages:\n",
        "  source_dir = f\"../MKA-200/{language}/intermediate_files/\"\n",
        "  dest_dir = f\"{language}/intermediate_files/\"\n",
        "\n",
        "  # Create destination directory if it doesn't exist\n",
        "  os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "  # Iterate through files in the source directory\n",
        "  for filename in os.listdir(source_dir):\n",
        "      source_path = os.path.join(source_dir, filename)\n",
        "      dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "      # Copy files\n",
        "      if os.path.isfile(source_path):\n",
        "          shutil.copy2(source_path, dest_path) # copy2 preserves metadata\n"
      ],
      "metadata": {
        "id": "ZaSmbvqZM1vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd MKA-SG-1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sehz59GnJbj3",
        "outputId": "a5a69b05-0b02-4589-c002-b7b7149796b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HAS/MKA-SG-1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_model = \"CohereForAI/aya-expanse-8b\"#\"google/gemma-2-2b-it\"#\"CohereForAI/aya-expanse-8b\"\n",
        "prompt_model_name = prompt_model.split(\"/\")[1]\n",
        "print(prompt_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4yAyB1gTC15",
        "outputId": "e973412c-1b50-4d0d-8358-ccbb4bb6d7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aya-expanse-8b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_models = [\"google/gemma-2-9b-it\", \"Qwen/Qwen2.5-7B-Instruct\", \"ssg97/gemma-2-27b-it-gptq-int4\"]"
      ],
      "metadata": {
        "id": "LbqtP0PN0JqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_models = [\"CohereForAI/aya-expanse-8b\", \"google/gemma-2-2b-it\", ]"
      ],
      "metadata": {
        "id": "FCq203vwSGCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "\n",
        "for prompt_model in prompt_models:\n",
        "  prompt_model_name = prompt_model.split(\"/\")[1]\n",
        "  print(prompt_model_name)\n",
        "\n",
        "  for tgt_lang, (lang, (prompts, options, answers)) in tgt_lang_data.items():\n",
        "    # if tgt_lang in [\"ben_Beng\"]:\n",
        "    #   continue\n",
        "    for task, aux_langs in aux_langs_dict.items():\n",
        "      answers = load_pickle(f'{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_prompt_answers.pkl')\n",
        "\n",
        "      print(tgt_lang, \" ==> \", aux_langs)\n",
        "      max_length = 384 if task == \"low_res\" else 256\n",
        "\n",
        "      # answers_only = extract_answers(answers, 6)\n",
        "\n",
        "      answer_translations = auxiliary_to_target(answers, aux_langs, max_length=max_length)\n",
        "\n",
        "      save_to_pickle(answer_translations, f\"{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_answer_translations.pkl\")\n",
        "\n",
        "      torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ZgSi6TH7RsiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def find_most_probable_answer(answers, n_gram_range=(2,3)):\n",
        "  vectorizer = CountVectorizer(analyzer='char', ngram_range=n_gram_range)\n",
        "  X = vectorizer.fit_transform(answers)\n",
        "\n",
        "  similarities = cosine_similarity(X)\n",
        "\n",
        "  avg_similarities = np.mean(similarities, axis=1)\n",
        "  most_central_idx = np.argmax(avg_similarities)\n",
        "\n",
        "  return most_central_idx"
      ],
      "metadata": {
        "id": "uGQy0zY36In_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(confidence_scores, ground_truths, confidence_cutoff):\n",
        "  n_samples = len(confidence_scores)\n",
        "  abstentions = sum(1 for conf in confidence_scores if conf < confidence_cutoff)\n",
        "  answered = n_samples - abstentions\n",
        "\n",
        "  true_confidences = sum(1 for conf, true in zip(confidence_scores, ground_truths) if true and conf >= confidence_cutoff)\n",
        "  false_confidences = sum(1 for conf, true in zip(confidence_scores, ground_truths) if not true and conf >= confidence_cutoff)\n",
        "  incorrectly_abstained = sum(1 for conf, true in zip(confidence_scores, ground_truths) if true and conf < confidence_cutoff)\n",
        "  correctly_abstained = sum(1 for conf, true in zip(confidence_scores, ground_truths) if not true and conf < confidence_cutoff)\n",
        "\n",
        "  answered_accuracy = true_confidences / answered if answered else 0\n",
        "  correctly_abstained_rate = correctly_abstained / abstentions if abstentions else 0\n",
        "  composite_accuracy = (true_confidences + correctly_abstained) / n_samples\n",
        "  answer_rate = answered/n_samples\n",
        "\n",
        "  conf_mean = np.mean(confidence_scores)\n",
        "  conf_std = np.std(confidence_scores)\n",
        "  conf_hist = np.histogram(confidence_scores, bins=10, range=(0,1))[0].tolist()\n",
        "\n",
        "  return {\n",
        "    \"abstention_metrics\": {\n",
        "      \"answered\": answered,\n",
        "      \"abstentions\": abstentions,\n",
        "      \"answer_rate\": answered/n_samples,\n",
        "      \"abstention_rate\": abstentions/n_samples,\n",
        "      \"correct_confidences\": true_confidences,\n",
        "      \"incorrect_confidences\": false_confidences,\n",
        "      \"correctly_abstained\": correctly_abstained,\n",
        "      \"incorrectly_abstained\": incorrectly_abstained,\n",
        "\n",
        "      \"answered_accuracy\": answered_accuracy,\n",
        "      \"correctly_abstained_rate\": correctly_abstained_rate,\n",
        "      \"accuracy\": true_confidences/n_samples,\n",
        "      # \"composite_accuracy\": composite_accuracy,\n",
        "      \"effective_accuracy\": composite_accuracy * answer_rate,\n",
        "\n",
        "      # \"total_accuracy\": (true_confidences + incorrectly_abstained)/n_samples,\n",
        "      # \"f1_score\": calculate_f1(correct_answers, incorrect_abstentions, false_confidences),\n",
        "    },\n",
        "    \"mean_confidence\": conf_mean,\n",
        "    \"confidence_std\": conf_std,\n",
        "    \"confidence_histogram\": conf_hist,\n",
        "  }"
      ],
      "metadata": {
        "id": "XFElRDidpaNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this function to use a different correct answer calibration\n",
        "def get_model_correctness(model_answers, true_answers):\n",
        "  return get_similarity(model_answers, true_answers) > 0.85\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, np.float32):\n",
        "      return float(obj)\n",
        "    if isinstance(obj, np.bool_):\n",
        "      return bool(obj)\n",
        "    return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "for prompt_model in prompt_models:\n",
        "  prompt_model_name = prompt_model.split(\"/\")[1]\n",
        "  print(prompt_model_name)\n",
        "\n",
        "  for i, (tgt_lang, (lang, (prompts, options, true_answers))) in enumerate(tgt_lang_data.items()):\n",
        "    # if tgt_lang in [\"ben_Beng\"]:\n",
        "    #   continue\n",
        "    for task, aux_langs in aux_langs_dict.items():\n",
        "      model_answers_translated = load_pickle(f'{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_answer_translations.pkl').values()\n",
        "      translated_prompts = load_pickle(f'{lang}/intermediate_files/{task}_{n_samples}_translated_prompts.pkl')\n",
        "      model_responses = load_pickle(f'{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_prompt_answers.pkl')\n",
        "      # model_responses = extract_answers(answers, 6)\n",
        "      prompts_list = [list(item) for item in zip(*translated_prompts)]\n",
        "\n",
        "      confidence_scores, ground_truths, samples = process_answers(\n",
        "          model_answers_translated,\n",
        "          prompts,\n",
        "          options,\n",
        "          true_answers,\n",
        "          prompts_list,\n",
        "          model_responses,\n",
        "          aux_langs\n",
        "      )\n",
        "\n",
        "      save_to_pickle(\n",
        "          (confidence_scores, ground_truths, samples),\n",
        "          f'{lang}/intermediate_files/{task}_{prompt_model_name}_{n_samples}_final_tuple.pkl'\n",
        "      )\n",
        "\n",
        "      runs = [{\n",
        "          \"run_id\": f\"{tgt_lang}_{int(time.time())}\",\n",
        "          \"target_language\": tgt_lang,\n",
        "          \"task\": task,\n",
        "          \"auxiliary_languages\": aux_langs,\n",
        "          \"confidence_cutoff\": confidence_cutoff,\n",
        "          \"metrics\": calculate_metrics(\n",
        "              confidence_scores,\n",
        "              ground_truths,\n",
        "              confidence_cutoff,\n",
        "          )\n",
        "      } for confidence_cutoff in np.arange(0, 1, 0.02)]\n",
        "\n",
        "      data = {\n",
        "        \"experiment_id\": \"high-res-v/s-low-res\",\n",
        "        \"metadata\": {\n",
        "            \"model\": prompt_model,\n",
        "            \"translation_model\": \"nllb-200-distilled-1.3B\",\n",
        "            # \"dataset\": \"mmlu\",\n",
        "            \"num_samples\": n_samples,\n",
        "        },\n",
        "        \"samples\": samples,\n",
        "        \"runs\": runs,\n",
        "      }\n",
        "\n",
        "      with open(f\"{lang}/results/{task}_{prompt_model_name}_{n_samples}_{similarity_model}.json\", \"w\") as f:\n",
        "        json.dump(data, f, cls=NumpyEncoder)\n",
        "\n",
        "print(\"Finished processing\")"
      ],
      "metadata": {
        "id": "aevEivnPTGfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n"
      ],
      "metadata": {
        "id": "K4gyMJvSVz18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/My Drive/HAS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAw3YzkHV7zv",
        "outputId": "4534d6d7-9729-430d-a2b8-e4e1da4da722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HAS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_map = {\n",
        "  \"aya-expanse-8b\": [\"Aya Expanse 8B\", \"-\", \"o\"],\n",
        "  \"gemma-2-9b-it\": [\"Gemma 2 9B\", \":\", \"^\"],\n",
        "  \"gemma-2-2b-it\": [\"Gemma 2 2B\", \"--\", \"s\"],\n",
        "  \"Qwen2.5-7B-Instruct\": [\"Qwen2.5 7B\", \"-.\", \"X\"],\n",
        "  \"DeepSeek-R1-Distill-Llama-8B\": [\"DeepSeek-R1 Llama 8B\", (0, (3, 1, 1, 1, 1, 1)), \"P\"],\n",
        "  \"gemma-2-27b-it-gptq-int4\": [\"Gemma 2 27B (int4)\", (0, (3, 1, 1, 1, 1, 1)), \"*\"]\n",
        "}"
      ],
      "metadata": {
        "id": "Tw_nIl9Ar4Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "language_colors = {\n",
        "        'Bengali': '#1f77b4',    # blue\n",
        "        'English': '#2ca02c',    # green\n",
        "        'Japanese': '#8c564b',     # brown\n",
        "        'Yoruba': '#9467bd',    # purple\n",
        "        'Indonesian': '#d62728',   # red\n",
        "        'Swahili': '#ff7f0e'  # orange\n",
        "    }\n",
        "\n",
        "def load_all_results(base_dir, models):\n",
        "    results_data = {}\n",
        "\n",
        "    # Iterate through all directories\n",
        "    for target_lang_dir in Path(base_dir).iterdir():\n",
        "        if not target_lang_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        results_dir = target_lang_dir / 'results'\n",
        "        if not results_dir.exists():\n",
        "            continue\n",
        "\n",
        "        target_lang = target_lang_dir.name\n",
        "        results_data[target_lang] = {}\n",
        "\n",
        "        # Load all JSON files in results directory\n",
        "        for result_file in results_dir.glob('*.json'):\n",
        "            split = result_file.stem.rsplit('_')\n",
        "            model_name = split[2]\n",
        "\n",
        "            if model_name not in models:\n",
        "              continue\n",
        "\n",
        "            with open(result_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            if not model_name in results_data[target_lang]:\n",
        "              results_data[target_lang][model_name] = {}\n",
        "\n",
        "            # Extract task type from filename\n",
        "            task_type = split[0]  # Assuming first part is task type\n",
        "\n",
        "            # Process runs data\n",
        "            runs = data['runs']\n",
        "            results_data[target_lang][model_name][task_type] = {\n",
        "                'confidence_cutoffs': [run['confidence_cutoff'] for run in runs],\n",
        "                'effective_accuracies': [run['metrics']['abstention_metrics']['effective_accuracy'] for run in runs],\n",
        "                'answered_accuracies': [run['metrics']['abstention_metrics']['answered_accuracy'] for run in runs],\n",
        "                'abstention_rates': [run['metrics']['abstention_metrics']['abstention_rate'] for run in runs],\n",
        "                'correctly_abstained_rates': [run['metrics']['abstention_metrics']['correctly_abstained_rate'] for run in runs],\n",
        "                'confidence_histograms': [run['metrics']['confidence_histogram'] for run in runs]\n",
        "            }\n",
        "\n",
        "    return results_data\n",
        "\n",
        "def plot_composite_accuracy_comparison(results_data, task_types=['low', 'mid', 'high']):\n",
        "    \"\"\"Plot composite accuracy across different task types, with separate graphs for each type.\"\"\"\n",
        "    # fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    # fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    gs = fig.add_gridspec(2, 24)\n",
        "\n",
        "    axes = [fig.add_subplot(gs[0, 0:11]), fig.add_subplot(gs[0, 13:]), fig.add_subplot(gs[1, 6:17]), fig.add_subplot(gs[1, 19:])]\n",
        "\n",
        "    plot_lines = []\n",
        "    models = []\n",
        "    lang = results_data.keys()\n",
        "\n",
        "    for idx, task_type in enumerate(task_types):\n",
        "      # plt.figure(figsize=(8, 8))\n",
        "      i = idx // 2\n",
        "      j = idx % 2\n",
        "\n",
        "      for target_lang, lang_data in results_data.items():\n",
        "        lines = []\n",
        "        for model, model_data in lang_data.items():\n",
        "          if task_type in model_data:\n",
        "            data = model_data[task_type]\n",
        "            l, = axes[idx].plot(\n",
        "                data['confidence_cutoffs'],\n",
        "                data['effective_accuracies'],\n",
        "                # label=f'{target_lang} {models_map[model][0]}',\n",
        "                color=language_colors[target_lang],\n",
        "                ls=models_map[model][1]\n",
        "                # marker=models_map[model][1]\n",
        "            )\n",
        "          lines.append(l)\n",
        "          models.append(models_map[model][0])\n",
        "        plot_lines.append(lines)\n",
        "\n",
        "      axes[idx].set_xlabel('Confidence Cutoff')\n",
        "      axes[idx].set_ylabel('Effective Accuracy')\n",
        "      axes[idx].grid(True)\n",
        "      axes[idx].set_title(f'{task_type.capitalize()} Resource')\n",
        "\n",
        "      # axes[i][j].set_xlabel('Confidence Cutoff')\n",
        "      # axes[i][j].set_ylabel('Effective Accuracy')\n",
        "      # axes[i][j].grid(True)\n",
        "      # axes[i][j].set_title(f'{task_type.capitalize()} Resource')\n",
        "      # plt.title(f'MKA Pipeline Performance on {task_type.capitalize()} Resource Auxiliary Languages')\n",
        "\n",
        "    legend1 = axes[3].legend(plot_lines[0], models, bbox_to_anchor=(0.3, 0.7), loc=\"center\")\n",
        "    axes[3].axis('off')\n",
        "    axes[3].legend([l[0] for l in plot_lines], lang, bbox_to_anchor=(0.3, 0.35), loc=\"center\")\n",
        "    plt.gca().add_artist(legend1)\n",
        "    # handles, labels = axes[0][0].get_legend_handles_labels()\n",
        "    # legend1 = axes[1][1].legend(handles, labels, loc='center')\n",
        "    # plt.suptitle(\"MKA Pipeline Accuracy v/s Confidence Cutoff (Gemma Family)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_metrics_dashboard(results_data, target_lang, model_name, task_type):\n",
        "    \"\"\"Create a dashboard of different metrics for a specific language and task.\"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    data = results_data[target_lang][model_name][task_type]\n",
        "\n",
        "    # Composite Accuracy\n",
        "    ax1.plot(data['confidence_cutoffs'], data['effective_accuracies'], 'b-', marker='o')\n",
        "    ax1.set_title('Effective Accuracy')\n",
        "    ax1.set_xlabel('Confidence Cutoff')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Answered Accuracy vs Abstention Rate\n",
        "    ax2.plot(data['confidence_cutoffs'], data['answered_accuracies'], 'g-', label='Answered Accuracy', marker='o')\n",
        "    ax2.plot(data['confidence_cutoffs'], data['abstention_rates'], 'r-', label='Abstention Rate', marker='o')\n",
        "    ax2.set_title('Answered Accuracy vs Abstention Rate')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Confidence Distribution Heatmap\n",
        "    sns.heatmap(data['confidence_histograms'], ax=ax3, cmap='YlOrRd')\n",
        "    ax3.set_title('Confidence Score Distribution')\n",
        "    ax3.set_xlabel('Confidence Bins')\n",
        "\n",
        "    # Correctly Abstained Rate\n",
        "    ax4.plot(data['confidence_cutoffs'], data['correctly_abstained_rates'], 'purple', marker='o')\n",
        "    ax4.set_title('Correctly Abstained Rate')\n",
        "    ax4.set_xlabel('Confidence Cutoff')\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.suptitle(f'Metrics Dashboard - {target_lang} ({task_type})')\n",
        "    plt.tight_layout()\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "def calculate_auc_metrics(results_data):\n",
        "    \"\"\"\n",
        "    Calculate AUC metrics for coverage vs accuracy curves.\n",
        "    Coverage = fraction of samples answered (1 - abstention_rate)\n",
        "    \"\"\"\n",
        "    auc_metrics = {}\n",
        "\n",
        "    for target_lang, lang_data in results_data.items():\n",
        "        auc_metrics[target_lang] = {}\n",
        "        for model, model_data in lang_data.items():\n",
        "          if model not in auc_metrics[target_lang]:\n",
        "            auc_metrics[target_lang][model] = {}\n",
        "          for task_type, task_data in model_data.items():\n",
        "            # Calculate coverage (x-axis)\n",
        "            coverage = 1 - np.array(task_data['abstention_rates'])\n",
        "            # Sort by coverage to ensure proper AUC calculation\n",
        "            sort_idx = np.argsort(coverage)\n",
        "            coverage_sorted = coverage[sort_idx]\n",
        "            accuracies_sorted_by_coverage_idx = np.array(task_data['answered_accuracies'])[sort_idx]\n",
        "\n",
        "            # Calculate AUC\n",
        "            auc_score = auc(coverage_sorted, accuracies_sorted_by_coverage_idx)\n",
        "            auc_metrics[target_lang][model][task_type] = auc_score\n",
        "\n",
        "            # Add to results_data for plotting\n",
        "            results_data[target_lang][model][task_type]['coverage'] = coverage.tolist()\n",
        "            results_data[target_lang][model][task_type]['auc_score'] = auc_score\n",
        "\n",
        "    return auc_metrics\n",
        "\n",
        "def plot_coverage_accuracy_curves(results_data, task_types=['low', 'mid', 'high']):\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    gs = fig.add_gridspec(2, 24)\n",
        "\n",
        "    axes = [fig.add_subplot(gs[0, 0:11]), fig.add_subplot(gs[0, 13:]), fig.add_subplot(gs[1, 6:17]), fig.add_subplot(gs[1, 19:])]\n",
        "\n",
        "    plot_lines = []\n",
        "    models = []\n",
        "    lang = results_data.keys()\n",
        "\n",
        "    for idx, task_type in enumerate(task_types):\n",
        "      # plt.figure(figsize=(9, 8))\n",
        "      for target_lang, lang_data in results_data.items():\n",
        "        lines = []\n",
        "        for model, model_data in lang_data.items():\n",
        "          if task_type in model_data:\n",
        "            data = model_data[task_type]\n",
        "            auc_score = data['auc_score']\n",
        "\n",
        "            points = np.array(list(zip(data['coverage'], data['answered_accuracies'])))\n",
        "            sorted_points = points[points[:, 0].argsort()]\n",
        "\n",
        "            l, = axes[idx].plot(\n",
        "                sorted_points[:, 0],  # sorted coverage\n",
        "                sorted_points[:, 1],  # corresponding accuracies\n",
        "                # label=f'{target_lang} {models_map[model][0]} (AUC={auc_score:.3f})',\n",
        "                color=language_colors[target_lang],\n",
        "                # ls=models_map[model][1]\n",
        "                marker=models_map[model][2],\n",
        "                linewidth=0.5,\n",
        "                markersize=5\n",
        "            )\n",
        "          lines.append(l)\n",
        "          models.append(models_map[model][0])\n",
        "\n",
        "        plot_lines.append(lines)\n",
        "\n",
        "        # plt.xlabel('Coverage (1 - Abstention Rate)')\n",
        "        # plt.ylabel('Accuracy')\n",
        "        # plt.title(f'{task_type.capitalize()} Resource')\n",
        "        # plt.grid(True)\n",
        "        # # plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
        "        #   # fancybox=True, shadow=False, ncol=2)\n",
        "\n",
        "      axes[idx].set_xlabel('Coverage (1 - Abstention Rate)')\n",
        "      axes[idx].set_ylabel('Accuracy')\n",
        "      axes[idx].grid(True)\n",
        "      axes[idx].set_title(f'{task_type.capitalize()} Resource')\n",
        "\n",
        "    legend1 = axes[3].legend(plot_lines[0], models, bbox_to_anchor=(0.3, 0.7), loc=\"center\")\n",
        "    axes[3].axis('off')\n",
        "    axes[3].legend([l[0] for l in plot_lines], lang, bbox_to_anchor=(0.3, 0.35), loc=\"center\")\n",
        "    plt.gca().add_artist(legend1)\n",
        "    # handles, labels = axes[0][0].get_legend_handles_labels()\n",
        "    # legend1 = axes[1][1].legend(handles, labels, loc='center')\n",
        "    # plt.suptitle(\"MKA Pipeline Accuracy v/s Coverage (Gemma Family)\")\n",
        "\n",
        "      # legend1 = axes[3].legend(plot_lines[0], models, loc='upper center', bbox_to_anchor=(0.3, -0.1))\n",
        "      # plt.legend([l[0] for l in plot_lines], language_colors.keys(), loc='upper center', bbox_to_anchor=(0.6, -0.1))\n",
        "      # plt.gca().add_artist(legend1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_language_family_clustering(results_data, language_families):\n",
        "    \"\"\"\n",
        "    Visualize performance clustering by language families.\n",
        "\n",
        "    language_families: dict mapping target_lang to its family\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Prepare data for clustering visualization\n",
        "    family_data = {}\n",
        "    for target_lang, lang_data in results_data.items():\n",
        "        family = language_families.get(target_lang, 'Unknown')\n",
        "        if family not in family_data:\n",
        "            family_data[family] = []\n",
        "\n",
        "        # Use average composite accuracy as metric\n",
        "        avg_accuracy = np.mean([\n",
        "            np.mean(task_data['effective_accuracies'])\n",
        "            for task_data in lang_data.values()\n",
        "        ])\n",
        "        family_data[family].append(avg_accuracy)\n",
        "\n",
        "    # Create violin plot\n",
        "    data_for_plot = []\n",
        "    labels = []\n",
        "    for family, accuracies in family_data.items():\n",
        "        data_for_plot.append(accuracies)\n",
        "        labels.append(family)\n",
        "\n",
        "    plt.violinplot(data_for_plot, showmeans=True)\n",
        "    plt.xticks(range(1, len(labels) + 1), labels, rotation=45)\n",
        "    plt.ylabel('Average Composite Accuracy')\n",
        "    plt.title('Performance Distribution by Language Family')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_performance_stability(results_data):\n",
        "    \"\"\"\n",
        "    Visualize stability of performance across confidence thresholds.\n",
        "    Uses standard deviation of accuracy as stability metric.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    stability_metrics = {}\n",
        "    for target_lang, lang_data in results_data.items():\n",
        "        for model, model_data in lang_data.items():\n",
        "          for task_type, task_data in model_data.items():\n",
        "              # Calculate rolling standard deviation\n",
        "              window = 5  # adjust as needed\n",
        "              accuracies = np.array(task_data['effective_accuracies'])\n",
        "              rolling_std = pd.Series(accuracies).rolling(window=window).std()\n",
        "\n",
        "              plt.plot(\n",
        "                  task_data['confidence_cutoffs'][window-1:],\n",
        "                  rolling_std[window-1:],\n",
        "                  label=f'{target_lang}-{task_type}',\n",
        "                  alpha=0.7\n",
        "              )\n",
        "\n",
        "    plt.xlabel('Confidence Cutoff')\n",
        "    plt.ylabel('Rolling Standard Deviation of Accuracy')\n",
        "    plt.title('Performance Stability Analysis')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_error_analysis(results_data, target_lang, model_name, task_type):\n",
        "    \"\"\"\n",
        "    Detailed error analysis visualization showing types of errors.\n",
        "    \"\"\"\n",
        "    data = results_data[target_lang][model_name][task_type]\n",
        "    confidence_cutoffs = np.array(data['confidence_cutoffs'])\n",
        "\n",
        "    # Find the point where abstention actually starts happening\n",
        "    abstention_rates = np.array(data['abstention_rates'])\n",
        "    abstention_start_idx = np.where(abstention_rates > 0)[0][0] if np.any(abstention_rates > 0) else len(abstention_rates)\n",
        "\n",
        "    # Create masked arrays for incorrect abstentions\n",
        "    incorrect_answers = 1 - np.array(data['answered_accuracies'])\n",
        "    incorrect_abstentions = 1 - np.array(data['correctly_abstained_rates'])\n",
        "\n",
        "    # Mask the pre-abstention region\n",
        "    masked_incorrect_abstentions = np.ma.array(incorrect_abstentions)\n",
        "    masked_incorrect_abstentions[:abstention_start_idx] = np.ma.masked\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Error composition with masked data\n",
        "    ax1.stackplot(\n",
        "        confidence_cutoffs,\n",
        "        [incorrect_answers, masked_incorrect_abstentions],\n",
        "        labels=['Incorrect Answers', 'Incorrect Abstentions']\n",
        "    )\n",
        "    ax1.set_xlabel('Confidence Cutoff')\n",
        "    ax1.set_ylabel('Error Rate')\n",
        "    ax1.set_title('Error Composition')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Error rate vs Coverage\n",
        "    coverage = 1 - abstention_rates\n",
        "    total_error_rate = (incorrect_answers * coverage +\n",
        "                       incorrect_abstentions * (1 - coverage))\n",
        "\n",
        "    ax2.scatter(coverage, total_error_rate, c=confidence_cutoffs,\n",
        "                cmap='viridis')\n",
        "    ax2.set_xlabel('Coverage')\n",
        "    ax2.set_ylabel('Total Error Rate')\n",
        "    ax2.set_title('Error Rate vs Coverage')\n",
        "    plt.colorbar(ax2.collections[0], ax=ax2, label='Confidence Cutoff')\n",
        "\n",
        "    plt.suptitle(f'Error Analysis - {target_lang} ({task_type})')\n",
        "    plt.tight_layout()\n",
        "\n",
        "# def plot_error_analysis(results_data, target_lang, task_type):\n",
        "#     \"\"\"\n",
        "#     Detailed error analysis visualization showing types of errors.\n",
        "#     \"\"\"\n",
        "#     data = results_data[target_lang][task_type]\n",
        "\n",
        "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "#     # Error composition\n",
        "#     incorrect_answers = 1 - np.array(data['answered_accuracies'])\n",
        "#     incorrect_abstentions = 1 - np.array(data['correctly_abstained_rates'])\n",
        "\n",
        "#     ax1.stackplot(\n",
        "#         data['confidence_cutoffs'],\n",
        "#         [incorrect_answers, incorrect_abstentions],\n",
        "#         labels=['Incorrect Answers', 'Incorrect Abstentions']\n",
        "#     )\n",
        "#     ax1.set_xlabel('Confidence Cutoff')\n",
        "#     ax1.set_ylabel('Error Rate')\n",
        "#     ax1.set_title('Error Composition')\n",
        "#     ax1.legend()\n",
        "\n",
        "#     # Error rate vs Coverage\n",
        "#     coverage = 1 - np.array(data['abstention_rates'])\n",
        "#     total_error_rate = (incorrect_answers * coverage +\n",
        "#                        incorrect_abstentions * (1 - coverage))\n",
        "\n",
        "#     ax2.scatter(coverage, total_error_rate, c=data['confidence_cutoffs'],\n",
        "#                 cmap='viridis')\n",
        "#     ax2.set_xlabel('Coverage')\n",
        "#     ax2.set_ylabel('Total Error Rate')\n",
        "#     ax2.set_title('Error Rate vs Coverage')\n",
        "#     plt.colorbar(ax2.collections[0], ax=ax2, label='Confidence Cutoff')\n",
        "\n",
        "#     plt.suptitle(f'Error Analysis - {target_lang} ({task_type})')\n",
        "#     plt.tight_layout()\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "# Load results and calculate AUC metrics\n",
        "results = load_all_results('path/to/base/directory')\n",
        "auc_metrics = calculate_auc_metrics(results)\n",
        "\n",
        "# Plot coverage-accuracy curves\n",
        "plot_coverage_accuracy_curves(results)\n",
        "\n",
        "# Plot language family clustering\n",
        "language_families = {\n",
        "    'english': 'Germanic',\n",
        "    'german': 'Germanic',\n",
        "    'french': 'Romance',\n",
        "    'spanish': 'Romance',\n",
        "    'chinese': 'Sinitic',\n",
        "    # Add more languages and their families\n",
        "}\n",
        "plot_language_family_clustering(results, language_families)\n",
        "\n",
        "# Plot stability analysis\n",
        "plot_performance_stability(results)\n",
        "\n",
        "# Plot error analysis for specific language and task\n",
        "plot_error_analysis(results, 'english', 'high_res')\n",
        "\"\"\"\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "# Load all results\n",
        "results = load_all_results('path/to/base/directory')\n",
        "\n",
        "# Plot composite accuracy comparison\n",
        "plot_composite_accuracy_comparison(results)\n",
        "\n",
        "# Create dashboard for specific language and task\n",
        "plot_metrics_dashboard(results, 'chinese', 'high_res')\n",
        "\n",
        "# Save plots\n",
        "plt.savefig('output_plot.png', dpi=300, bbox_inches='tight')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "r2S9tiCcoI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models = [\"aya-expanse-8b\", \"gemma-2-9b-it\", \"Qwen2.5-7B-Instruct\",]# \"DeepSeek-R1-Distill-Llama-8B\"]\n",
        "models = [\"gemma-2-9b-it\", \"Qwen2.5-7B-Instruct\", \"gemma-2-2b-it\", \"gemma-2-27b-it-gptq-int4\"]"
      ],
      "metadata": {
        "id": "0024ulyBTbC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = load_all_results('MKA-SG-1000', models)"
      ],
      "metadata": {
        "id": "guY34G_ghtUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_composite_accuracy_comparison(results)"
      ],
      "metadata": {
        "id": "N6cUY6sWiEQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics_dashboard(results, 'Bengali', 'aya-expanse-8b', 'high')\n",
        "plot_metrics_dashboard(results, 'English', 'aya-expanse-8b', 'high')\n",
        "plot_metrics_dashboard(results, 'Yoruba', 'aya-expanse-8b', 'high')"
      ],
      "metadata": {
        "id": "JfDqSmkOyq3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load results and calculate AUC metrics\n",
        "# results = load_all_results('MKA-200')\n",
        "auc_metrics = calculate_auc_metrics(results)\n",
        "\n",
        "# Plot coverage-accuracy curves\n",
        "plot_coverage_accuracy_curves(results)\n",
        "\n",
        "# # Plot language family clustering\n",
        "# language_families = {\n",
        "#     'english': 'Germanic',\n",
        "#     'german': 'Germanic',\n",
        "#     'french': 'Romance',\n",
        "#     'spanish': 'Romance',\n",
        "#     'chinese': 'Sinitic',\n",
        "#     # Add more languages and their families\n",
        "# }\n",
        "# plot_language_family_clustering(results, language_families)"
      ],
      "metadata": {
        "id": "XrdykMPu6Kw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot stability analysis\n",
        "# plot_performance_stability(results)\n",
        "\n",
        "# # Plot error analysis for specific language and task\n",
        "# plot_error_analysis(results, 'Bengali', 'aya-expanse-8b', 'high')\n",
        "# plot_error_analysis(results, 'English', 'aya-expanse-8b', 'high')\n",
        "# plot_error_analysis(results, 'Yoruba', 'aya-expanse-8b', 'high')\n",
        "# plot_error_analysis(results, 'Bengali', 'aya-expanse-8b', 'mid')"
      ],
      "metadata": {
        "id": "f3fmH9wf4npH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hGAULR9wBLzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JP2diLCU1NEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpLiC64H1NHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}